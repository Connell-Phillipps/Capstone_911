# **NYC 911 Call Volume Forecasting**
Author: Connell Phillipps <br><br>
==================================


## 1 - **Project Overview**
The goal of this project is to highlight and demonstrate the skills learned through BratinStations Data Science bootcamp. In this project, I used data provided by NYC Open Data to create a predictive algorithm to show 911 call hot spots. The goal, to create a tool that could be used to station NYC service units in the most ideal locations to reduce response time and maximize unit placement efficiency. Broken down by NYC police precinct and providing a general overview of the entire NYC region, this project can create a heat map to demonstrate 911 call hot spots.

### 1.1 - Problem Area
When 911 calls come in, they can vary in importance and urgency. However, when a serious call is made, it is the goal of 911 operators to efficiently assign the closest responder to the situation. It can sometimes be a challenge to coordinate and ensure assets are allocated in the appropriate region to assure minimum response time. By using a trained algorithm, this project could cut down on response time and alleviate stress on already overworked 911 responders by ensuring the correct unit is always in the most optimal area for any given hour of any given day.

### 1.2 - Those affected
The directly affected parties are those who would use this every day to assign service vehicle locations. However, the greater community would see the effect of this algorithm through increased response time, potentially leading to saved lives in cases where urgency is the top priority.

### 1.3 - Data Science Solution
I'm not 100% sure on the models to use here. My main idea is to create a grid of latitude and longitude within NYC bounds (excluding rivers or waterways) and calculate the probability of a call originating from that location at any given day and time. The second model could be used as a broader scale allocation tool to try to predict the number of calls within each NYC precinct region to assess at what day and time resources should be more efficiently assigned.

### 1.4 - Impact of Solution
According to this [article](https://patch.com/new-york/new-york-city/nypds-slow-response-times-keep-growing-longer-data-shows) by Matt Troutman on 2/2/2024, NYC response times have been increasing, with an average police response time of 16 minutes. Even if this tool could help reduce this amount of time by a fraction, the impact could lead to saved lives. For me, the cost of a life is immeasurable and thus gives me great motivation to create a successful predictive model.


### 1.5 - Dataset
For access to the dataset used in this project visit:<br> 
https://data.cityofnewyork.us/Public-Safety/NYPD-Calls-for-Service-Year-to-Date-/n2zq-pubd/about_data<br><br>
If you would like to follow along with this project you will need to be careful to change the path when initializing the dataframe at the begining of each notebook. <br><br>
<img src=".\misc\initdf1.png" height="400">
<img src=".\misc\initdf2.png" height="400"><br>
This data set was provided from NYC Open Data, a relatively clean data set that is up to date and large enough to create an insightful resources.
A future iteration could pull the latest dataset from NYC Open Data's website so the infroming dataset is alwasy populated with the most recent call entries.

#### 1.5.1 - Data Dictionary
|**Column Name**|**Description**|**API Field Name**|**Data Type**|
|---|---|---|---|
|`GEO_CD_Y`|The Y-Coordinate of the midblock of the street segment where the violation was issued|geo_cd_y|text|
|`RADIO_CODE`|NYPD code used to inform NYPD member of service the nature of the call|radio_code|text|
|`TYP_DESC`|Description based on RADIO_CODE|typ_desc|text|
|`CIP_JOBS`|Flag indicating if the call relates tto a Crime in Progress (CIP)|cip_jobs|text|
|`ADD_TS`|Timestamp of when the call was added to the system|add_ts|Floating Timestamp|
|`DISP_TS`|Timestamp of when the call was dispatched to a responding unit| disp_ts|Floating Timestamp|
|`ARRIVD_TS`|Timestamp of when the responding unit arrived on the scene|arrivd_ts|Floating Timestamp|
|`CLOSNG_TS`|Timestamp of when the call was marked closed|closng_ts|Floating Timestamp|
|`Latitude`|The latitude of the midblock of the street segment where the violation was issued|latitude|Number|
|`Longitude`|The Longitude of the midblock of the street segment where the violation was issued|longitude|Number|
|`CAD_EVNT_ID`|Unique identifier generated by the ICAD 911 system|cad_evnt_id|text|
|`CREATE_DATE`|Date of call|create_date|Floating Timestamp|
|`INCIDENT_DATE`|Date of incident|incident_date|Floating Timestamp|
|`INCIDENT_TIME`|Time of incident|incident_time|text|
|`NYPD_PCT_CD`|NYPD precinct calls is in| nypd_pct_cd|Number|

## 2 - Demo
1. Download the project file from github
1. Import the project enviornment to create a conda enviorment to run all files. Then activate the enviorment. In terminal copy the commands below
    - ```conda env create --name environment --file=environments.yml```
    - ```conda activate environment``` 
1. In terminal navigate to the project file and then into the streamlit folder
1. Run a local instance of the streamlit app to play around with
    - ```streamlit run app.py```

From here you are free to play around with the different tabs and discover more about the data for yourself!<br><br>
Here are some exsamples of the working website:

<img src=".\misc\home.png" height="400"><br>
Home Page<br><br>
<img src=".\misc\precinct_timeseries_heatmap.png" height="400"><br>
Heatmap visualization<br><br>
<img src=".\misc\deep_dive.png" height="400"><br>
Deep Dive Data Investigation


## 3 - Project Organization
* `notebooks` - The file contains all the working jupyter notebooks. These notebooks are the foundation of the project and if you would like to follow along I suggest starting there.
    - `api_requesting` - This is the first step for the project. Here the data is pulled from NYC open data's website api and saved into a local csv to be imported in the following notebooks. Be careful this step can take an extremely long time as there are over 40 million rows to pull.
    - `cleaning` - This notebook deals with cleaning the data. This consists of eliminating duplicate rows, redundant columns, and assigning data types to more efficiently store the data. This steps cuts the massive 12 GB file down to 3 GB and makes the rest of the project much easier. This is also the notebooks that deals with adding any engineered features such as response time to incident.
    - `eda` - This notebook is the first look at the data, this is where insights and investigation into the larger data set take place. Here distributions comparisons are made to determine data consistency and inconsistencies to gain insights.
    - `predictive_modeling` - This notebooks acts as the bulk of this project and is where the predictions are made. First by choosing a model based off RMSE then by running a separate model across all locations to predicts call volume by hour. This notebook details the steps that lead me to choosing the facebook prophet model for time series forecasting. The output is a .pkl file that contains a dictionary of each location with the actual and predicted call volumes by hour.
    - `visualizations` - This is the pretty end product notebook that showcases the data forecasted above and displays in in a stunning heatmap for user consumption and understanding. Another warning here that the data takes a while to run through the heatmap and displaying calls by hour over more then a few days can take a very long time for the graphs to populate.
* `reports` - This file contains the different presentation made during the progression of this project up to the final presentation which was given during a BrainStation Demo day. This is a great non-technical way to gain an understanding of the project and the steps taken to ultimately achieve the final product.
* `sprints` - This file contains the different save points in the project. Sprint 1 being the first submission for this capstone project with sprint 3 being only 2 weeks before the final presentation. This again is more of a save state hold for anyone interested in the development of this project.
* `streamlit` - This file contains what I consider the final product of this project. All steps before this are saved to a streamlit website which you can download and run to view the product displayed in the demo above. The Streamlit app is the most user friendly way to investigate the call data and the forecasting that was made. This would be my ideal tool for the NYPD to use to better assign there resources to reduce call response time.

## 4- Next Steps

- [] Create a smaller subset of locationality for lat/lon to more accurately predict call locations
- [] Further investigate full data set, its very large and there are more insights to be gathered in EDA
- [] find and attach a population dataset for better investigation per capita
    - [] Find a economic dataset by location to even further investigate economic call distribution by number/type of calls
- [] When NYC open data updates call volumes update and re-run models to predict further into the future.
    - [] Compare actual calls vs forecasted calls by previous model