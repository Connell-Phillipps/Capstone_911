{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling by Precinct:**\n",
    "\n",
    "The purpose of this notebook is to import the cleaned data from the EDA notebook and build a model to predict the number of 911 calls per precinct by any given hour.<br><br>\n",
    "\n",
    "The first part of this notebook will be creating a subset of the data aggregated by hour and precinct/burrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# plotting\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "#import libraries used in this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#stats\n",
    "from statsmodels.api import tsa\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa import stattools\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "#Facebook Profit\n",
    "from prophet import Prophet\n",
    "\n",
    "#Math\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data and cleaning index\n",
    "df_hourly = pd.read_csv(r\"C:\\Users\\cmphi\\Documents\\BrainStation\\DataBases\\capstone_911\\hourly_df.csv\", parse_dates=['Unnamed: 0'])\n",
    "df_hourly.set_index(\"Unnamed: 0\", inplace=True)\n",
    "df_hourly.index.name = None\n",
    "df_hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Modeling:**\n",
    "\n",
    "Now we can move to modeling a forecast for each time breakdown.\n",
    "\n",
    "**Step 1: Trend-Seasonal Decomposition**\n",
    "\n",
    "A fundamental step in time series EDA is the trend-seasonal decomposition. Here, we extract three series from our original observation: \n",
    "- a trend component $T_t$ calculated using a moving average,\n",
    "- a seasonal component $S_t$ which is the monthly/daily average of the de-trended series, and\n",
    "- the residual $R_t$ that remains after subtracting the trend and seasonal component from the original series.\n",
    "\n",
    "Adding up these three components will give back the original series:\n",
    "\n",
    "$$y_t = T_t + S_t + R_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to demonstrate the process and assure the data and predations look correct we will predict the total number of 911 calls for every hour in 2024 and compare the to known values. Once satisfied withe the process we will create a function that takes a database (`df_hourly`) and makes a prediction for every column so we can predict the number of calls each precinct will receive.\n",
    "\n",
    "In order for the model to run faster we will be taking a snapshot of the data and only try to forecast number of calls using the most resent year of data. This should not only improve the runtime of the model but also ensure the forecast is only focusing on the most informative information. To be sure we have the smallest data set that still captures has a high predictive accuracy we will test out a few time frames. The shortest being a dataset of 4 weeks, and the longest being a dataset of 15 months.\n",
    "\n",
    "Below I will be displaying one example of what the code is doing but will be executing the code for each of the different sized datasets in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = df_hourly.index.max()\n",
    "print(last_index - timedelta(hours=(15*672)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the first run of the parameter search for the SARIMAX model we will be narrowing down the size of the dataset to allow for faster running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regifining code for ease of updated data\n",
    "last_index = df_hourly.index.max()\n",
    "hours_per_month = 672 #hours in 4 weeks\n",
    "\n",
    "#4 month dataset\n",
    "df_hourly_4m = df_hourly[['total']].loc[(df_hourly.index >= (last_index - timedelta(hours=(4*hours_per_month))))]\n",
    "\n",
    "#3 month dataset\n",
    "df_hourly_3m = df_hourly[['total']].loc[(df_hourly.index >= (last_index - timedelta(hours=(3*hours_per_month))))]\n",
    "\n",
    "#2 month dataset\n",
    "df_hourly_2m = df_hourly[['total']].loc[(df_hourly.index >= (last_index - timedelta(hours=2*hours_per_month)))]\n",
    "\n",
    "#creating a list of datasets\n",
    "dfs = [df_hourly_2m, df_hourly_3m, df_hourly_4m]\n",
    "dfs_names = ['2_month', '3_month', '4_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through decomposition\n",
    "for df in dfs:\n",
    "\n",
    "    #decomposition\n",
    "    decomposition = tsa.seasonal_decompose(df['total'], model='additive', period=24)\n",
    "\n",
    "    #creating columns for each decomp\n",
    "    df[\"Trend\"] = decomposition.trend\n",
    "    df[\"Seasonal\"] = decomposition.seasonal\n",
    "    df[\"Residual\"] = decomposition.resid\n",
    "\n",
    "dfs[0].head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting decomposition of 1 month dataset\n",
    "cols = [\"Trend\", \"Seasonal\", \"Residual\"]\n",
    "\n",
    "# Create subplots with increased vertical spacing\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1, subplot_titles=cols,\n",
    "    vertical_spacing=0.1, shared_xaxes=True  # Ensure shared x-axes\n",
    ")\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=dfs[0].index, y=dfs[0][col]),\n",
    "        row=i+1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "# Set the range slider to be visible only on the last subplot\n",
    "fig.update_xaxes(rangeslider_visible=False, row=1, col=1)\n",
    "fig.update_xaxes(rangeslider_visible=False, row=2, col=1)\n",
    "fig.update_xaxes(rangeslider_visible=True, row=3, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,  # Increase the height to allow space for the range slider\n",
    "    width=1100,\n",
    "    showlegend=False,\n",
    "    title_text=\"Trend, Seasonal, and Residual Components\",\n",
    "    margin=dict(t=50, b=50)  # Add margin to avoid overlapping with the range slider\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does seem to be some slight trend with in the data so we will remove this before we start modeling as the model we will be using does best with a stationary series. This differencing will be added back later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train and Testing split:**\n",
    "\n",
    "Next we will be splitting the data into a train and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(dfs[0])*.75)\n",
    "dfs[0].iloc[:495].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train(80%) and test(20%) and test. Also, we drop the null values introduced at differencing\n",
    "for df in dfs:\n",
    "    \n",
    "    #creating index count for 80%\n",
    "    train_size = int(len(df) * 0.8)\n",
    "\n",
    "    #creating training row of False\n",
    "    df['train'] = False\n",
    "\n",
    "    #editing training index rows to True\n",
    "    df.iloc[:train_size, df.columns.get_loc('train')] = True\n",
    "\n",
    "#check to show head is True training and tail is False training\n",
    "display(dfs[0].head())\n",
    "display(dfs[0].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create the baseline prediction that is the just the mean of the training set as a jumping off point for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the RSME of models\n",
    "rmse_df = pd.DataFrame()\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    # Calculate the mean of the training data to use as the baseline\n",
    "    baseline_value = np.mean(df[df['train']]['total'])\n",
    "\n",
    "    # Create a baseline series filled with the mean value\n",
    "    baseline = np.full(df.shape[0], baseline_value)\n",
    "    \n",
    "    # Create a predictions series with the same index as df\n",
    "    predictions = pd.Series(baseline, index=df.index)\n",
    "\n",
    "    train_rmse = root_mean_squared_error(df[df['train']]['total'], predictions[df[df['train']].index])\n",
    "    test_rmse = root_mean_squared_error(df[~df['train']]['total'], predictions[df[~df['train']].index])\n",
    "\n",
    "    ## #create rsme dataframe for test and train\n",
    "    \n",
    "    rmse_df.loc[dfs_names[i], 'Baseline_train'] = train_rmse\n",
    "    rmse_df.loc[dfs_names[i], 'Baseline_test'] = test_rmse\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        #Plotting baseline\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df[df['train']].index, y=df[df['train']]['total'], mode='lines', name=\"Train\"))\n",
    "        fig.add_trace(go.Scatter(x=df[~df['train']].index, y=df[~df['train']]['total'], mode='lines', name=\"Test\"))\n",
    "        fig.add_trace(go.Scatter(x=predictions.index, y=predictions, mode='lines', name=\"Mean Prediction\"))\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis_title=\"Difference number of calls\", \n",
    "            xaxis_title=\"Date\",\n",
    "            title=\"Change in Number of Calls over Prior Hour (total)\"\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(rangeslider_visible=True)\n",
    "        fig.show()\n",
    "\n",
    "print('RMSE:')\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric we will be using to evalute the models will be the _Root Mean Squared Error_ (RMSE), this allows us to look at the Square root of the error the prediction is off by. We now have benchmarks for each of the different time length dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before modeling will be to choose what lags we will be using to forecast on. Below is a graph showing the confidence interval of each lag with a larger value representing confidence the lag is significant as related to the predicted lag. The graph below is just a visual representation of what lags we will be picking from. To do this more manually we will graph the index of every lag above a specified confidence inteval (.08), this value was chosen in an iterative process starting form .05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plot_pacf(df_hourly_2m[\"total\"], lags=172, ax=plt.gca(), method='ywm', alpha=0.05, zero=False)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial AC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am choosing 172 lags to look at. This is derived from the EDA notebook where we saw deviation from the mean in terms of hour of the day and day of the week, but less in terms of month of the year and year in general. So 172 lags is a week (in terms of hours) and a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to pull index of lagged values that have highest PACF\n",
    "def get_lag_index(pacf_values, n=10, confidence_interval=0.05):\n",
    "    # Calculate absolute PACF values\n",
    "    abs_pacf = np.absolute(pacf_values)\n",
    "    \n",
    "    # Get the indices of the top n+1 values (including the 0th lag which will be removed)\n",
    "    top_n_indices = np.argpartition(abs_pacf, -n-1)[-n-1:]\n",
    "    \n",
    "    # Remove the 0th lag (if it exists) as it's not a lagged value\n",
    "    top_n_indices = top_n_indices[top_n_indices != 0]\n",
    "    \n",
    "    # Filter out indices where PACF value is less than the confidence interval\n",
    "    top_n_indices = [i for i in top_n_indices if abs_pacf[i] >= confidence_interval]\n",
    "    \n",
    "    # Sort the indices to maintain the original order\n",
    "    top_n_indices.sort()\n",
    "    \n",
    "    # Return only the top n indices\n",
    "    return top_n_indices[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous runs of the parameter search below we can better determine the `indicies_range` that will yield the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through different time set dataframes and modeling the SARIMAX scores\n",
    "indices_range = [3,4,9,10,11]\n",
    "\n",
    "#looping through all index ranges and time datasets\n",
    "for n in indices_range:\n",
    "    #for display to see how loop is running\n",
    "    print(f'Number of top Indicies: {n}')\n",
    "    for i, df in enumerate(dfs):\n",
    "        # Grabbing significant lags\n",
    "        pacf_total = stattools.pacf(df[df['train']]['total'], nlags=172, method='ywm').tolist()\n",
    "\n",
    "        #saving n number of influential lags\n",
    "        p_params = get_lag_index(pacf_total, n)\n",
    "\n",
    "        #for display to see how loop is running\n",
    "        print(f'{dfs_names[i]}:', end='\\r')\n",
    "\n",
    "        ##Modeling##\n",
    "        model = SARIMAX(df[df['train']]['total'], order=(p_params, 0, 0), trend=\"c\", enforce_stationarity=False)\n",
    "        model_fit = model.fit(dsip=False)\n",
    "\n",
    "        #predicting and saving\n",
    "        df[f'SARIMAX_predictions_{n}'] = model_fit.predict(start=0, end=len(df))\n",
    "\n",
    "        #finding the rmse for train and test\n",
    "        train_rmse = root_mean_squared_error(df[df['train']]['total'], df[f'SARIMAX_predictions_{n}'][df['train']])\n",
    "        test_rmse = root_mean_squared_error(df[~df['train']]['total'], df[f'SARIMAX_predictions_{n}'][~df['train']])\n",
    "\n",
    "\n",
    "        ###create rsme dataframe for test and train###\n",
    "        rmse_df.loc[dfs_names[i], f'SARIMAX_train_{n}'] = format(train_rmse, '.3f')\n",
    "        rmse_df.loc[dfs_names[i], f'SARIMAX_test_{n}'] = format(test_rmse, '.3f')\n",
    "\n",
    "    print('RMSE:')\n",
    "    display(rmse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the parameter search above we want to minsize RMSE while also picking model that will run quickly for every location group we pass in. With that being said, the larger the training data the better the forecast is at predicting father into the future. Meaning the data set given to the model needs to be updated less often to assure good predictions.\n",
    "\n",
    "<br>From that it appears that a 3 month dataset and 11 of the top most influential lags. Show below are the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting best prediction\n",
    "df = dfs[1] #picking 3 month period\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df[df['train']].index, y=df[df['train']]['total'], mode='lines', name=\"Train\"))\n",
    "fig.add_trace(go.Scatter(x=df[~df['train']].index, y=df[~df['train']]['total'], mode='lines', name=\"Test\"))\n",
    "fig.add_trace(go.Scatter(x=df['SARIMAX_predictions_11'].index, y=df['SARIMAX_predictions_11'], mode='lines', name=\"Mean Prediction\"))\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Difference number of calls\", \n",
    "    xaxis_title=\"Date\",\n",
    "    title=\"Change in Number of Calls over Prior Hour (total)\"\n",
    ")\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a bit deeper dive into the prediction made above it looks as if the most accurate forecasting will be made about a week after then end of the training data. After that we see the model becomes more cautious on its predictions and starts to smooth its predictive curve. This means that the best model for predicting using the above method would need to be undated and retrained every week to predict for the next week. If this model were to be seriously implemented it would need to be someones job to update this model once a week at the end of the week to predict for the next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Differencing:**\n",
    "Next, we want to see if removing any trend will help the model. Usually this step is conducted if during the decomposition stage we see a uniform trend. However, in our case there wasn't really a consistent trend. Still we will compare and see if it will help the accuracy of the results. As there is a 24 hour cycle in a day we will use that to construct the differencing, meaning we are comparing number of call as compared to the day before and conducting the SARIMAX model on the difference value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing trend with a 24 hour period of differencing\n",
    "for df in dfs:\n",
    "    df['hourly_difference'] = df['total'].diff(24)\n",
    "\n",
    "    #dropping starting differences as they will have NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    #Repeating decomposition\n",
    "    decomposition = tsa.seasonal_decompose(df['hourly_difference'], model='additive')\n",
    "\n",
    "    #adding decomp back in\n",
    "    df[\"d_Trend\"] = decomposition.trend\n",
    "    df[\"d_Seasonal\"] = decomposition.seasonal\n",
    "    df[\"d_Residual\"] = decomposition.resid\n",
    "\n",
    "dfs[0].head(13)\n",
    "\n",
    "#Plotting difference decomp for 1 month dataset\n",
    "# Create subplots with increased vertical spacing\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1, subplot_titles=cols,\n",
    "    vertical_spacing=0.1, shared_xaxes=True  # Ensure shared x-axes\n",
    ")\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=dfs[0].index, y=dfs[0][col]),\n",
    "        row=i+1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "# Set the range slider to be visible only on the last subplot\n",
    "fig.update_xaxes(rangeslider_visible=False, row=1, col=1)\n",
    "fig.update_xaxes(rangeslider_visible=False, row=2, col=1)\n",
    "fig.update_xaxes(rangeslider_visible=True, row=3, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,  # Increase the height to allow space for the range slider\n",
    "    width=1100,\n",
    "    showlegend=False,\n",
    "    title_text=\"Trend, Seasonal, and Residual Components\",\n",
    "    margin=dict(t=50, b=50)  # Add margin to avoid overlapping with the range slider\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in indices_range:\n",
    "    print(f'Number of top Indices: {n}')\n",
    "    for i, df in enumerate(dfs):\n",
    "        print(f'{dfs_names[i]}:', end='\\r')\n",
    "\n",
    "        # Grabbing significant lags\n",
    "        pacf_total = stattools.pacf(df[df['train']]['hourly_difference'], nlags=172, method='ywm').tolist()\n",
    "        p_params = get_lag_index(pacf_total, n)\n",
    "\n",
    "        # Modeling step\n",
    "        model = SARIMAX(df[df['train']]['hourly_difference'], order=(p_params, 0, 0), trend=\"c\", enforce_stationarity=False)\n",
    "        model_fit = model.fit(disp=0)\n",
    "\n",
    "        # Making predictions\n",
    "        df[f'SARIMAX_predictions_d{n}'] = model_fit.predict(start=0, end=len(df) - 1)\n",
    "\n",
    "        # Adding back in differencing\n",
    "        first_day_indices = df.index[:24]  # 24 here is equal to the period of differencing\n",
    "        remaining_indices = df.index[24:]\n",
    "\n",
    "        # Create an empty restored column\n",
    "        df['restored'] = np.nan\n",
    "\n",
    "        # Filling in the first 24 original values into the empty restored column\n",
    "        df.loc[first_day_indices, \"restored\"] = df.loc[first_day_indices, 'total']\n",
    "\n",
    "        # Use the current difference value and 24-hour lagged restored value to get the next restored value\n",
    "        for current_date in remaining_indices:\n",
    "            current_value = df.loc[current_date, \"hourly_difference\"]\n",
    "            previous_restored = df.loc[current_date - pd.DateOffset(hours=24), \"restored\"]\n",
    "            df.loc[current_date, \"restored\"] = previous_restored + current_value\n",
    "\n",
    "        # use the original train diff values and predicted test values\n",
    "        df.loc[df[df['train']].index, \"AR_difference\"] = df[f'SARIMAX_predictions_d{n}'][df['train']]\n",
    "        df.loc[df[~df['train']].index, \"AR_difference\"] = df[f'SARIMAX_predictions_d{n}'][~df['train']]\n",
    "\n",
    "        # empty restored column\n",
    "        df[\"AR_restored\"] = np.nan\n",
    "\n",
    "        # fill in the first 12 original values\n",
    "        df.loc[first_day_indices, \"AR_restored\"] = df.loc[first_day_indices, \"total\"]\n",
    "\n",
    "        # use the current difference and 12-month lagged restored value to get the next restored\n",
    "        for current_date in remaining_indices:\n",
    "            current_value = df.loc[current_date, \"AR_difference\"]\n",
    "            day_before_restored = df.loc[current_date - pd.DateOffset(hours=24), \"AR_restored\"]\n",
    "\n",
    "            df.loc[current_date, \"AR_restored\"] = day_before_restored + current_value\n",
    "\n",
    "        # Calculate RMSE for train and test\n",
    "        train_rmse = root_mean_squared_error(df[df['train']]['total'], df['AR_restored'][df['train']])\n",
    "        test_rmse = root_mean_squared_error(df[~df['train']]['total'], df['AR_restored'][~df['train']])\n",
    "\n",
    "        # Create RMSE DataFrame for test and train\n",
    "        rmse_df.loc[dfs_names[i], f'SARIMAX_train_d{n}'] = format(train_rmse, '.3f')\n",
    "        rmse_df.loc[dfs_names[i], f'SARIMAX_test_d{n}'] = format(test_rmse, '.3f')\n",
    "\n",
    "    display(rmse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting best prediction\n",
    "df = dfs[2] #picking 3 month period\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df[df['train']].index, y=df[df['train']]['total'], mode='lines', name=\"Train\"))\n",
    "fig.add_trace(go.Scatter(x=df[~df['train']].index, y=df[~df['train']]['total'], mode='lines', name=\"Test\"))\n",
    "fig.add_trace(go.Scatter(x=df['SARIMAX_predictions_d10'].index, y=df['AR_restored'], mode='lines', name=\"Mean Prediction\"))\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Difference number of calls\", \n",
    "    xaxis_title=\"Date\",\n",
    "    title=\"Change in Number of Calls over Prior Hour (total)\"\n",
    ")\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Facebook Profit:**\n",
    "Next we will be using the new facebook profit timeseries predictive model. It is supposed to be fast accurate and fully automatic. We will see and compare to Sarimax and differenced SARIMAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
